import os
import feedparser
import google.generativeai as genai
from supabase import create_client
from dotenv import load_dotenv
import datetime
import time
import json
import requests
import random
import dashscope # é˜¿é‡Œæ¨¡å‹åº“
from http import HTTPStatus

# ================= 1. é…ç½®åŒº =================
load_dotenv()

# ä»£ç†è®¾ç½® (Googleéœ€è¦)
os.environ["HTTP_PROXY"] = "http://127.0.0.1:7897"
os.environ["HTTPS_PROXY"] = "http://127.0.0.1:7897"

# åˆå§‹åŒ– Supabase
supabase = create_client(os.environ.get("SUPABASE_URL"), os.environ.get("SUPABASE_KEY"))

# åˆå§‹åŒ– Google Gemini (ä¸»åŠ›)
genai.configure(api_key=os.environ.get("GEMINI_API_KEY"))
# ä½¿ç”¨æ˜¨å¤©æµ‹è¯•é€šè¿‡çš„åˆ«åï¼Œé˜²æ­¢404
gemini_model = genai.GenerativeModel('gemini-flash-latest') 

# åˆå§‹åŒ– é˜¿é‡Œé€šä¹‰åƒé—® (å¤‡èƒ)
dashscope.api_key = os.environ.get("DASHSCOPE_API_KEY")

# ================= 2. æ‰å¹³åŒ–èœå•ç»“æ„ =================
MENU_STRUCTURE = {
    "æ”¿æ²»": [
        "https://www.zaobao.com.sg/rss/news/china",       # è”åˆæ—©æŠ¥
        "http://feeds.bbci.co.uk/news/world/rss.xml",      # BBC
        "http://rss.sina.com.cn/news/china/focus15.xml"   # æ–°æµª
    ],
    "ç»æµ": [
        "http://www.caixin.com/rss/finance.xml",          # è´¢æ–°
        "https://www.yicai.com/rss/toutiao.xml",           # ç¬¬ä¸€è´¢ç»
        "https://www.cnbc.com/id/10000664/device/rss/rss.html"
    ],
    "ç§‘æŠ€": [
        "https://www.36kr.com/feed",                      # 36Kr
        "https://www.theverge.com/rss/index.xml",         # The Verge
        "https://sspai.com/feed"                          # å°‘æ•°æ´¾
    ],
    "AI": [
        "https://www.jiqizhixin.com/rss",                 # æœºå™¨ä¹‹å¿ƒ
        "https://techcrunch.com/category/artificial-intelligence/feed/",
        "https://www.qbitai.com/feed"
    ],
    "æ®µå­": [] # AI åˆ›ä½œ
}

# ================= 3. æ ¸å¿ƒå·¥å…·å‡½æ•° =================

def fetch_rss_with_headers(url):
    """ä¼ªè£…æµè§ˆå™¨æŠ“å– RSS (è§£å†³æ— ç´ æé—®é¢˜)"""
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    try:
        # è®¾ç½®15ç§’è¶…æ—¶
        response = requests.get(url, headers=headers, timeout=15)
        response.encoding = 'utf-8'
        return feedparser.parse(response.content)
    except Exception as e:
        print(f"    âš ï¸ ç½‘ç»œæŠ“å–å¤±è´¥: {e}")
        return None

def clean_text(text):
    from bs4 import BeautifulSoup
    try:
        return BeautifulSoup(text, "html.parser").get_text()[:300]
    except:
        return text[:300]

# --- æ™ºèƒ½ AI è°ƒç”¨å‡½æ•° (åŒ…å«å¤‡èƒé€»è¾‘) ---
def call_ai_smart(prompt, return_json=False):
    """
    å°è¯•è°ƒç”¨ Geminiï¼Œå¦‚æœå¤±è´¥è‡ªåŠ¨åˆ‡æ¢åˆ°é˜¿é‡Œ Qwen
    return_json: æ˜¯å¦å¼ºåˆ¶è¦æ±‚è¿”å› JSON æ ¼å¼
    """
    # 1. å°è¯• Gemini
    try:
        # print("     ğŸ¤– å‘¼å« Google Gemini...")
        response = gemini_model.generate_content(prompt)
        text = response.text.strip()
        # å¦‚æœéœ€è¦JSONï¼Œå°è¯•è§£æä¸€ä¸‹ï¼Œè§£æå¤±è´¥ä¹Ÿç®—å¤±è´¥ï¼Œè½¬ç»™é˜¿é‡Œ
        if return_json:
            clean_json = text.replace("```json", "").replace("```", "").strip()
            json.loads(clean_json) # æµ‹è¯•è§£æ
        return text
    
    except Exception as e:
        print(f"     âš ï¸ Gemini é‡åˆ°å›°éš¾ ({e})ï¼Œåˆ‡æ¢é˜¿é‡Œé€šä¹‰åƒé—®...")
        
        # 2. åˆ‡æ¢ Qwen (å¤‡èƒ)
        try:
            # ä¸´æ—¶å…³é—­ä»£ç†ï¼Œå› ä¸ºé˜¿é‡Œåœ¨å›½å†…ç›´è¿æ›´å¿«
            proxies = os.environ.copy()
            if "HTTP_PROXY" in os.environ: del os.environ["HTTP_PROXY"]
            if "HTTPS_PROXY" in os.environ: del os.environ["HTTPS_PROXY"]
            
            response = dashscope.Generation.call(
                model=dashscope.Generation.Models.qwen_turbo,
                prompt=prompt
            )
            
            # æ¢å¤ä»£ç†
            os.environ["HTTP_PROXY"] = "http://127.0.0.1:7897"
            os.environ["HTTPS_PROXY"] = "http://127.0.0.1:7897"

            if response.status_code == HTTPStatus.OK:
                return response.output.text.strip()
            else:
                print(f"     âŒ é˜¿é‡Œ Qwen ä¹ŸæŠ¥é”™äº†: {response.message}")
                return None
        except Exception as qwen_e:
            print(f"     âŒ å¤‡ç”¨æ¨¡å‹è°ƒç”¨å¤±è´¥: {qwen_e}")
            return None

# ================= 4. ä¸šåŠ¡é€»è¾‘ =================

# --- ç”Ÿæˆæ¯æ—¥å“²ç† ---
def generate_daily_quote():
    today_str = datetime.datetime.now().strftime('%Y-%m-%d')
    print("âœ¨ æ­£åœ¨ç”Ÿæˆä»Šæ—¥å“²ç†/å†å² (åŒæ¨¡ç‰ˆ)...")
    
    # æŸ¥é‡
    existing = supabase.table("daily_quotes").select("id").eq("date", today_str).execute()
    if existing.data:
        print("   å·²å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
        return

    prompt = f"""
    ä»Šå¤©æ˜¯ {today_str}ã€‚è¯·éšæœºç”Ÿæˆä¸€ä¸ªå¯Œæœ‰å“²ç†çš„å†…å®¹ã€‚
    å¯ä»¥æ˜¯ä»¥ä¸‹ä¸¤ç§ä¹‹ä¸€ï¼ˆéšæœºé€‰ä¸€ä¸ªï¼‰ï¼š
    1. å†å²ä¸Šçš„ä»Šå¤©å‘ç”Ÿçš„æœ‰è¶£æˆ–æœ‰æ·±æ„çš„äº‹æƒ…ï¼Œå¹¶é™„å¸¦ç®€çŸ­è¯„è®ºã€‚
    2. ä¸€å¥åäººåè¨€ï¼Œå¹¶é™„å¸¦å¯Œæœ‰æ·±åº¦çš„ç°ä»£è§£è¯»ã€‚
    
    è¦æ±‚ï¼š
    - ä¸¥æ ¼è¿”å› JSON æ ¼å¼ï¼Œä¸è¦å¤šä½™åºŸè¯ï¼š{{"content": "ä¸»è¦å†…å®¹", "author": "ä½œè€…æˆ–å†å²äº‹ä»¶æ ‡é¢˜"}}
    - å­—æ•°æ§åˆ¶åœ¨ 150 å­—ä»¥å†…ã€‚
    - è¯­æ°”ä¼˜ç¾ã€æœ‰å¯å‘æ€§ã€‚
    """
    
    result_text = call_ai_smart(prompt, return_json=True)
    
    if result_text:
        try:
            clean_text = result_text.replace("```json", "").replace("```", "").strip()
            data = json.loads(clean_text)
            
            supabase.table("daily_quotes").insert({
                "date": today_str,
                "content": data.get("content"),
                "author": data.get("author", "å†å²ä¸Šçš„ä»Šå¤©")
            }).execute()
            print("   âœ… å“²ç†å…¥åº“æˆåŠŸ")
        except Exception as e:
            print(f"   âŒ å“²ç†æ ¼å¼è§£æå¤±è´¥: {e}")

# --- ç”Ÿæˆæ–°é—»æ¿å— ---
def generate_news_brief(category, feeds):
    print(f"\nğŸ“‚ å¤„ç†æ¿å—: {category} ...")
    today_str = datetime.datetime.now().strftime('%Y-%m-%d')
    
    if category == "æ®µå­":
        prompt = """
        è¯·å†™ä¸€æ®µâ€œæ¯æ—¥ä¸€ç¬‘â€ã€‚
        è¦æ±‚ï¼šæ”¶é›†3-5ä¸ªå¥½ç¬‘çš„æ®µå­ã€ç¥å›å¤æˆ–è€…èŒåœº/ç§‘æŠ€åœˆå†·ç¬‘è¯ã€‚
        é£æ ¼ï¼šå¹½é»˜ã€é€šä¿—ã€è§£å‹ã€‚
        æ€»å­—æ•°ï¼š300å­—å·¦å³ã€‚
        """
        links_data = []
    else:
        # æŠ“å–æ–°é—»
        articles = []
        for url in feeds:
            print(f"   è¯»å–: {url} ...")
            feed = fetch_rss_with_headers(url) # ä½¿ç”¨å¸¦ä¼ªè£…çš„æŠ“å–
            if feed and feed.entries:
                for entry in feed.entries[:3]:
                    title = entry.title
                    desc = clean_text(entry.get('summary', '') or entry.get('description', ''))
                    articles.append(f"æ ‡é¢˜ï¼š{title}\nå†…å®¹ï¼š{desc}")
        
        if not articles:
            print("   âš ï¸ æ— æœ‰æ•ˆç´ æï¼Œè·³è¿‡")
            return

        # éšæœºæ‰“ä¹±å¹¶æˆªå–ï¼Œé˜²æ­¢æ¯æ¬¡éƒ½ä¸€æ ·
        random.shuffle(articles)
        combined_text = "\n\n".join(articles[:8]) 
        
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šä¸»ç¼–ã€‚è¯·æ ¹æ®ä»¥ä¸‹ç´ æï¼Œå†™ä¸€ç¯‡ã€{category}ã€‘æ¿å—çš„ç®€æŠ¥ã€‚
        
        è¦æ±‚ï¼š
        1. æŒ‘é€‰æœ€é‡è¦çš„5ä»¶äº‹ï¼ˆç´ æä¸è¶³åˆ™å…¨å†™ï¼‰ã€‚
        2. å°†å®ƒä»¬èåˆæˆä¸€ç¯‡é€šé¡ºã€æœ‰æ·±åº¦çš„æ–‡ç« ï¼ˆçº¦300-400å­—ï¼‰ã€‚
        3. æ¯æ®µä¹‹é—´é€»è¾‘æ¸…æ™°ã€‚å¦‚æœæ˜¯ç§‘æŠ€äº§å“ï¼Œé‡ç‚¹ä»‹ç»åŠŸèƒ½ã€‚
        4. è¯­æ°”ç°ä»£ã€ç®€æ´ã€ä¸“ä¸šã€‚
        5. åªè¿”å›çº¯æ–‡æœ¬ã€‚
        
        ç´ æï¼š
        {combined_text}
        """
        # è®°å½•ç¬¬ä¸€æ¡é“¾æ¥ä½œä¸ºå‚è€ƒï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        links_data = [{"title": "ç‚¹å‡»æŸ¥çœ‹ä»Šæ—¥ç›¸å…³çƒ­é—¨æºæ–‡", "url": feeds[0]}] if feeds else []

    # è°ƒç”¨ AI (è‡ªåŠ¨åŒæ¨¡åˆ‡æ¢)
    content = call_ai_smart(prompt)
    
    if content:
        # å…¥åº“ (main_menu å›ºå®šä¸º 'å…¨ç«™' ä»¥é€‚é…æ–°é€»è¾‘)
        data = {
            "date": today_str,
            "main_menu": "å…¨ç«™",
            "sub_menu": category,
            "content": content,
            "links": links_data
        }
        supabase.table("daily_briefs").insert(data).execute()
        print(f"   âœ… [{category}] å…¥åº“æˆåŠŸ")
        
        print("   â˜• ä¼‘æ¯ 10 ç§’...")
        time.sleep(10)

# ================= ä¸»ç¨‹åº =================
if __name__ == "__main__":
    generate_daily_quote()
    for cat, feeds in MENU_STRUCTURE.items():
        generate_news_brief(cat, feeds)
    print("\nğŸ‰ æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼")