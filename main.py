import os
import feedparser
import google.generativeai as genai
from supabase import create_client
from dotenv import load_dotenv
import datetime
import time
import json
import requests
import random
import dashscope
from http import HTTPStatus
import re # ç”¨äºæå–å›¾ç‰‡

# ================= 1. é…ç½®åŒº =================
load_dotenv()
# os.environ["HTTP_PROXY"] = "http://127.0.0.1:7897" # ä½ çš„VPNç«¯å£
# os.environ["HTTPS_PROXY"] = "http://127.0.0.1:7897"

supabase = create_client(os.environ.get("SUPABASE_URL"), os.environ.get("SUPABASE_KEY"))
# å¼•å…¥ transport æ¨¡å—
from google.api_core import client_options as client_options_lib

# é…ç½®è¶…æ—¶æ—¶é—´ä¸º 10 ç§’ (è€Œä¸æ˜¯é»˜è®¤çš„ 600 ç§’)
genai.configure(
    api_key=os.environ.get("GEMINI_API_KEY"),
    transport="rest", # å¼ºåˆ¶ä½¿ç”¨ REST åè®®ï¼Œæœ‰æ—¶å€™èƒ½è§£å†³ gRPC è¿æ¥é—®é¢˜
    client_options=client_options_lib.ClientOptions(
        api_endpoint="generativelanguage.googleapis.com"
    )
)
gemini_model = genai.GenerativeModel('gemini-flash-latest') 
dashscope.api_key = os.environ.get("DASHSCOPE_API_KEY")

# ================= 2. æ‰©å……åçš„ä¿¡æº (æ›´ç¨³å®š) =================
RSS_SOURCES = {
    "æ”¿æ²»": [
        "https://www.zaobao.com.sg/rss/news/china",       # è”åˆæ—©æŠ¥
        "http://feeds.bbci.co.uk/news/world/rss.xml",      # BBC
        "http://rss.sina.com.cn/news/china/focus15.xml"   # æ–°æµªå›½å†…è¦é—» (é‡å¤§ç®¡é¥±)
    ],
    "ç»æµ": [
        "http://www.caixin.com/rss/finance.xml",          # è´¢æ–°
        "https://www.yicai.com/rss/toutiao.xml",           # ç¬¬ä¸€è´¢ç»
        "http://rss.sina.com.cn/news/finance/chinalist.xml", # æ–°æµªè´¢ç» (è¡¥å……æº)
        "https://feed.36kr.com/tags/finance"                # 36æ°ªé‡‘è
    ],
    "ç§‘æŠ€": [
        "https://www.36kr.com/feed",                      # 36Kr
        "https://sspai.com/feed",                         # å°‘æ•°æ´¾
        "https://www.huxiu.com/rss/0.xml"                 # è™å—… (é«˜è´¨é‡ç§‘æŠ€è¯„è®º)
    ],
    "AI": [
        "https://www.jiqizhixin.com/rss",                 # æœºå™¨ä¹‹å¿ƒ
        "https://www.qbitai.com/feed",                    # é‡å­ä½
        "https://rsshub.app/36kr/search/article/AI"       # 36Kr AIæ ‡ç­¾ (å¤‡ç”¨)
    ]
}

# ================= 3. å›¾ç‰‡å…œåº•åº“ (å¦‚æœæŠ“ä¸åˆ°å›¾ï¼Œä»è¿™é‡Œéšæœºé€‰) =================
FALLBACK_IMAGES = {
    "æ”¿æ²»": [
        "https://images.unsplash.com/photo-1529101091760-6149d4c46b29?w=800&q=80",
        "https://images.unsplash.com/photo-1575517111839-3a3843ee7f5d?w=800&q=80",
        "https://images.unsplash.com/photo-1451187580459-43490279c0fa?w=800&q=80"
    ],
    "ç»æµ": [
        "https://images.unsplash.com/photo-1611974765270-ca1258634369?w=800&q=80",
        "https://images.unsplash.com/photo-1590283603385-17ffb3a7f29f?w=800&q=80",
        "https://images.unsplash.com/photo-1526304640152-d4619684e484?w=800&q=80"
    ],
    "ç§‘æŠ€": [
        "https://images.unsplash.com/photo-1518770660439-4636190af475?w=800&q=80",
        "https://images.unsplash.com/photo-1550751827-4bd374c3f58b?w=800&q=80",
        "https://images.unsplash.com/photo-1531297424005-063400c61634?w=800&q=80"
    ],
    "AI": [
        "https://images.unsplash.com/photo-1677442136019-21780ecad995?w=800&q=80",
        "https://images.unsplash.com/photo-1620712943543-bcc4688e7485?w=800&q=80",
        "https://images.unsplash.com/photo-1555255707-c07966088b7b?w=800&q=80"
    ],
    "æ®µå­": [
        "https://images.unsplash.com/photo-1505664194779-8beaceb93744?w=800&q=80",
        "https://images.unsplash.com/photo-1520607162513-77705c0f0d4a?w=800&q=80"
    ]
}

# ================= 4. æ ¸å¿ƒå·¥å…·å‡½æ•° =================

def fetch_rss_with_headers(url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/91.0.4472.124 Safari/537.36'}
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.encoding = 'utf-8'
        return feedparser.parse(response.content)
    except:
        return None

def clean_text(text):
    from bs4 import BeautifulSoup
    try:
        return BeautifulSoup(text, "html.parser").get_text()[:300]
    except:
        return text[:300]

def extract_image(entry):
    """å°è¯•ä» RSS æ¡ç›®ä¸­æå–å›¾ç‰‡ URL"""
    # 1. å°è¯• media_content
    if 'media_content' in entry:
        return entry.media_content[0]['url']
    # 2. å°è¯• links
    if 'links' in entry:
        for link in entry.links:
            if 'image' in link.type:
                return link.href
    # 3. å°è¯•ä» description çš„ HTML ä¸­æ‰¾ <img src="...">
    if 'summary' in entry:
        match = re.search(r'src="(http.*?jpg|png|jpeg)"', entry.summary)
        if match:
            return match.group(1)
    return None

def get_random_image(category):
    """å¦‚æœæ²¡æŠ“åˆ°å›¾ï¼Œéšæœºå‘ä¸€å¼ å¥½çœ‹çš„"""
    images = FALLBACK_IMAGES.get(category, FALLBACK_IMAGES["ç§‘æŠ€"])
    return random.choice(images)

# --- æ™ºèƒ½ AI è°ƒç”¨ ---
def call_ai_smart(prompt, return_json=False):
    # å°è¯• Gemini
    try:
        response = gemini_model.generate_content(prompt)
        text = response.text.strip()
        if return_json:
            clean = text.replace("```json", "").replace("```", "").strip()
            return json.loads(clean)
        return text
    except Exception as e:
        print(f"     âš ï¸ Gemini å¤±è´¥ ({e})ï¼Œåˆ‡æ¢ Qwen...")
        try:
            # å…³ä»£ç†è°ƒ Qwen
            proxies = os.environ.copy()
            if "HTTP_PROXY" in os.environ: del os.environ["http://127.0.0.1:7897"]
            if "HTTPS_PROXY" in os.environ: del os.environ["http://127.0.0.1:7897"]
            
            response = dashscope.Generation.call(
                model=dashscope.Generation.Models.qwen_turbo,
                prompt=prompt
            )
            # æ¢å¤ä»£ç†
            os.environ["HTTP_PROXY"] = "http://127.0.0.1:7897"
            os.environ["HTTPS_PROXY"] = "http://127.0.0.1:7897"

            if response.status_code == HTTPStatus.OK:
                text = response.output.text.strip()
                if return_json:
                    clean = text.replace("```json", "").replace("```", "").strip()
                    return json.loads(clean)
                return text
        except:
            return None

# ================= 5. ä¸šåŠ¡é€»è¾‘ =================

def generate_daily_quote():
    today_str = datetime.datetime.now().strftime('%Y-%m-%d')
    print("âœ¨ ç”Ÿæˆä»Šæ—¥å“²ç†...")
    if supabase.table("daily_quotes").select("id").eq("date", today_str).execute().data:
        print("   å·²å­˜åœ¨ï¼Œè·³è¿‡")
        return

    prompt = f"""
    ä»Šå¤©æ˜¯ {today_str}ã€‚è¯·ç”Ÿæˆä¸€æ¡å†…å®¹ï¼š
    1. å†å²ä¸Šçš„ä»Šå¤©å‘ç”Ÿçš„æ·±æ„äº‹ä»¶ã€‚
    2. æˆ–ä¸€å¥å¯Œæœ‰å“²ç†çš„åäººåè¨€ã€‚
    è¦æ±‚ï¼šJSONæ ¼å¼ {{"content": "å†…å®¹+è§£è¯»", "author": "ä½œè€…/äº‹ä»¶"}}ï¼Œ150å­—å†…ã€‚
    """
    data = call_ai_smart(prompt, return_json=True)
    if data:
        supabase.table("daily_quotes").insert({
            "date": today_str, "content": data.get("content"), "author": data.get("author")
        }).execute()
        print("   âœ… å“²ç†å…¥åº“")

def generate_category_cards(category):
    today_str = datetime.datetime.now().strftime('%Y-%m-%d')
    print(f"\nğŸ“‚ å¤„ç†åˆ†ç±»: {category} ...")

    materials = []
    # ç”¨äºä¸´æ—¶å­˜å‚¨å›¾ç‰‡æ˜ å°„ {æ ‡é¢˜: å›¾ç‰‡URL}
    image_map = {} 

    if category == "æ®µå­":
        # å‡çº§ç‰ˆæ®µå­æç¤ºè¯
        prompt_sys = """
        ä½ æ˜¯ä¸€ä¸ªçŠ€åˆ©çš„è„±å£ç§€æ¼”å‘˜ã€‚è¯·åˆ›ä½œ5ä¸ªæ®µå­ã€‚
        è¦æ±‚ï¼š
        1. åŒ…å«ï¼šèŒåœºåæ§½ã€ç§‘æŠ€åœˆæ€ªç°çŠ¶ã€æˆ–è€…ç”Ÿæ´»ç¥å›å¤ã€‚
        2. é£æ ¼ï¼šè¦å¥½ç¬‘ã€ç¨å¾®å¸¦ç‚¹è®½åˆºã€æ‹’ç»è€æ¢—ã€‚
        3. æ ¼å¼ï¼šæ¯ä¸ªæ®µå­ç‹¬ç«‹æˆæ®µã€‚
        """
        # æ®µå­ä¸éœ€è¦æŠ“å– RSS
    else:
        feeds = RSS_SOURCES.get(category, [])
        count = 0
        for url in feeds:
            feed = fetch_rss_with_headers(url)
            if feed and feed.entries:
                for entry in feed.entries[:4]: # æ¯ä¸ªæºå¤šå–ç‚¹ï¼Œé˜²æ­¢é‡å¤
                    title = entry.title
                    # å°è¯•æå–å›¾ç‰‡ï¼Œæå–ä¸åˆ°å°±ç”¨å…œåº•å›¾
                    img = extract_image(entry) or get_random_image(category)
                    image_map[title] = img
                    
                    materials.append(f"æ ‡é¢˜ï¼š{title}\né“¾æ¥ï¼š{entry.link}\næ‘˜è¦ï¼š{clean_text(entry.get('summary',''))}")
                    count += 1
        
        if not materials:
            print("   âš ï¸ æ— ç´ æï¼Œè·³è¿‡")
            return
        
        random.shuffle(materials)
        materials = materials[:12] # ç»™ AI å–‚12æ¡
        prompt_sys = f"ä½ æ˜¯ä¸€ä¸ªæ–°é—»ç¼–è¾‘ã€‚æ ¹æ®ç´ ææ€»ç»“5æ¡æœ€æœ‰ä»·å€¼çš„ã€{category}ã€‘æ–°é—»ã€‚æ¯æ¡300å­—ï¼Œå®¢è§‚ç®€æ˜ã€‚"

    # æ„å»º Prompt
    prompt = f"""
    {prompt_sys}
    
    ã€é‡è¦ã€‘è¯·ä¸¥æ ¼è¿”å› JSON æ•°ç»„æ ¼å¼ï¼Œä¸è¦ Markdownã€‚
    æ ¼å¼ï¼š
    [
        {{"title": "åŸæ ‡é¢˜(å¿…é¡»ä¸ç´ æä¸­ä¸€è‡´)", "content": "300å­—æ‘˜è¦...", "url": "åŸå§‹é“¾æ¥", "source": "æ¥æºåª’ä½“"}}
    ]
    
    ç´ æå¦‚ä¸‹ï¼š
    {chr(10).join(materials)}
    """

    cards_json = call_ai_smart(prompt, return_json=True)
    
    if cards_json:
        # åå¤„ç†ï¼šæŠŠæˆ‘ä»¬åˆšæ‰åœ¨ Python é‡Œå‡†å¤‡å¥½çš„å›¾ç‰‡å¡è¿›å»
        final_cards = []
        for card in cards_json:
            # å°è¯•é€šè¿‡æ ‡é¢˜åŒ¹é…å›¾ç‰‡ (æ¨¡ç³ŠåŒ¹é…ï¼Œåªè¦æ ‡é¢˜åŒ…å«åŸæ ‡é¢˜çš„ä¸€éƒ¨åˆ†å³å¯)
            # å¦‚æœæ˜¯æ®µå­ï¼Œç›´æ¥éšæœºé…å›¾
            if category == "æ®µå­":
                card['image'] = get_random_image("æ®µå­")
            else:
                # é»˜è®¤å›¾
                card['image'] = get_random_image(category)
                # å°è¯•æ‰¾å›çœŸå®å›¾
                for raw_title, raw_img in image_map.items():
                    if card.get('title') and (card['title'] in raw_title or raw_title in card['title']):
                        card['image'] = raw_img
                        break
            
            final_cards.append(card)

        # å­˜å…¥æ•°æ®åº“
        supabase.table("daily_briefs").insert({
            "date": today_str,
            "category": category,
            "cards": final_cards
        }).execute()
        print(f"   âœ… [{category}] å¡ç‰‡å…¥åº“æˆåŠŸ (å«å›¾ç‰‡)")
        time.sleep(5)

def generate_home_summary():
    today_str = datetime.datetime.now().strftime('%Y-%m-%d')
    print("\nğŸ  ç”Ÿæˆé¦–é¡µæ€»ç»“...")
    
    all_materials = []
    for cat, feeds in RSS_SOURCES.items():
        for url in feeds:
            feed = fetch_rss_with_headers(url)
            if feed and feed.entries:
                all_materials.append(f"[{cat}] {feed.entries[0].title}")
    
    if not all_materials: return

    prompt = f"""
    ä»Šå¤©æ˜¯ {today_str}ã€‚æ ¹æ®ä»¥ä¸‹æ ‡é¢˜å†™ä¸€æ®µ200å­—çš„å…¨ç«™ç»¼è¿°ã€‚
    è¦æ±‚ï¼šæœ‰æ·±åº¦ã€ç²¾è¾Ÿã€é€‚åˆåšå¯¼è¯»ã€‚åªè¿”å›çº¯æ–‡æœ¬ã€‚
    ç´ æï¼š{chr(10).join(all_materials[:15])}
    """
    
    summary = call_ai_smart(prompt)
    if summary:
        supabase.table("daily_briefs").insert({
            "date": today_str, "category": "é¦–é¡µ", "summary": summary
        }).execute()
        print("   âœ… é¦–é¡µæ€»ç»“å…¥åº“")

if __name__ == "__main__":
    generate_daily_quote()
    
    # æ¸…ç†å½“å¤©æ—§æ•°æ®(é˜²æ­¢é‡å¤)ï¼Œå¯é€‰
    # today = datetime.datetime.now().strftime('%Y-%m-%d')
    # supabase.table("daily_briefs").delete().eq("date", today).execute()

    for cat in RSS_SOURCES.keys():
        generate_category_cards(cat)
    generate_category_cards("æ®µå­")
    generate_home_summary()
    print("\nğŸ‰ å®Œæˆ")