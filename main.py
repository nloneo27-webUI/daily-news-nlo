import os
import feedparser
import google.generativeai as genai
from supabase import create_client
from dotenv import load_dotenv
import datetime
import time
import json
import requests
import dashscope # å¼•å…¥é˜¿é‡Œæ¨¡å‹åº“
from http import HTTPStatus

# ================= 1. é…ç½®åŒº =================
load_dotenv()

# ä»£ç†è®¾ç½® (Googleéœ€è¦ï¼Œé˜¿é‡Œä¸éœ€è¦ï¼Œä½†æŒ‚ç€ä¹Ÿæ— æ‰€è°“)
os.environ["HTTP_PROXY"] = "http://127.0.0.1:7897"
os.environ["HTTPS_PROXY"] = "http://127.0.0.1:7897"

# åˆå§‹åŒ– Supabase
supabase = create_client(os.environ.get("SUPABASE_URL"), os.environ.get("SUPABASE_KEY"))

# åˆå§‹åŒ– Google Gemini
genai.configure(api_key=os.environ.get("GEMINI_API_KEY"))
gemini_model = genai.GenerativeModel('gemini-flash-latest') # ä¸»åŠ›æ¨¡å‹

# åˆå§‹åŒ– é˜¿é‡Œé€šä¹‰åƒé—®
dashscope.api_key = os.environ.get("DASHSCOPE_API_KEY") # å¤‡ç”¨æ¨¡å‹ Key

# ================= 2. èœå•ç»“æ„ (ä¿æŒä¸å˜) =================
MENU_STRUCTURE = {
    "å›½å†…": {
        "æ”¿æ²»": ["https://www.zaobao.com.sg/rss/news/china", "http://rss.sina.com.cn/news/china/focus15.xml"],
        "ç»æµ": ["http://www.caixin.com/rss/finance.xml", "https://www.yicai.com/rss/toutiao.xml"],
        "ç§‘æŠ€": ["https://www.36kr.com/feed", "https://www.yicai.com/rss/kechuang.xml"],
        "AI": ["https://www.jiqizhixin.com/rss", "https://www.qbitai.com/feed"]
    },
    "å›½é™…": {
        "æ”¿æ²»": ["http://feeds.bbci.co.uk/news/world/rss.xml"],
        "ç»æµ": ["https://www.cnbc.com/id/10000664/device/rss/rss.html"],
        "ç§‘æŠ€": ["https://www.theverge.com/rss/index.xml"],
        "AI": ["https://techcrunch.com/category/artificial-intelligence/feed/"]
    },
    "åˆ›æ„": {
        "ç§‘æŠ€äº§å“": ["https://www.producthunt.com/feed"],
        "æ¯æ—¥ä¸€ç¬‘": [] 
    }
}

# ================= 3. æ ¸å¿ƒå·¥å…·å‡½æ•° =================

def fetch_rss_with_headers(url):
    """ä¼ªè£…æµè§ˆå™¨æŠ“å– RSS"""
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.encoding = 'utf-8'
        return feedparser.parse(response.content)
    except Exception as e:
        print(f"    âš ï¸ ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
        return None

def clean_text(text):
    from bs4 import BeautifulSoup
    try:
        return BeautifulSoup(text, "html.parser").get_text()[:500]
    except:
        return text[:500]

# --- æ–°å¢ï¼šä¸“é—¨è°ƒç”¨é˜¿é‡Œ Qwen çš„å‡½æ•° ---
def call_qwen_model(prompt):
    print("     ğŸ›¡ï¸ [å¤‡èƒå¯åŠ¨] åˆ‡æ¢åˆ°é˜¿é‡Œé€šä¹‰åƒé—® (Qwen-Turbo)...")
    try:
        # ä¸´æ—¶å…³é—­ä»£ç†ï¼Œå› ä¸ºé˜¿é‡Œåœ¨å›½å†…ç›´è¿æ›´å¿« (å¯é€‰ï¼Œä¸å…³ä¹Ÿèƒ½é€š)
        # os.environ.pop("HTTP_PROXY", None)
        # os.environ.pop("HTTPS_PROXY", None)

        response = dashscope.Generation.call(
            model=dashscope.Generation.Models.qwen_turbo,
            prompt=prompt
        )
        
        # æ¢å¤ä»£ç† (å¦‚æœåˆšæ‰å…³äº†çš„è¯)
        # os.environ["HTTP_PROXY"] = "http://127.0.0.1:7897"
        # os.environ["HTTPS_PROXY"] = "http://127.0.0.1:7897"

        if response.status_code == HTTPStatus.OK:
            return response.output.text
        else:
            print(f"     âŒ Qwen æŠ¥é”™: {response.code} - {response.message}")
            return None
    except Exception as e:
        print(f"     âŒ Qwen è°ƒç”¨å¤±è´¥: {e}")
        return None

def generate_brief_smart(main_cat, sub_cat, articles):
    """
    æ™ºèƒ½æ€»ç»“å‡½æ•°ï¼šä¼˜å…ˆç”¨ Geminiï¼Œå¤±è´¥è‡ªåŠ¨åˆ‡ Qwen
    """
    print(f"     âš¡ æ­£åœ¨ç”Ÿæˆ [{main_cat}-{sub_cat}] çš„æ€»ç»“...")
    
    # 1. æ„å»º Prompt (æç¤ºè¯)
    if sub_cat == "æ¯æ—¥ä¸€ç¬‘":
        prompt = "è¯·å†™ä¸€æ®µâ€œæ¯æ—¥ä¸€ç¬‘â€ï¼ŒåŒ…å«3ä¸ªå¹½é»˜æ®µå­ï¼Œæ€»å­—æ•°300å­—å·¦å³ã€‚"
        links_data = []
    else:
        combined_text = ""
        links_data = []
        for i, art in enumerate(articles):
            combined_text += f"ã€æ–°é—»{i+1}ã€‘æ ‡é¢˜ï¼š{art['title']}\nå†…å®¹æ‘˜è¦ï¼š{art['summary']}\n\n"
            links_data.append({"title": art['title'], "url": art['link']})
            
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªèµ„æ·±æ–°é—»ä¸»ç¼–ã€‚è¯·æ ¹æ®ä»¥ä¸‹ {len(articles)} æ¡ç´ æï¼Œå†™ä¸€ç¯‡300å­—çš„ç»¼è¿°ã€‚
        è¦æ±‚ï¼šä¸è¦ç½—åˆ—ï¼Œèåˆæˆé€šé¡ºæ–‡ç« ã€‚æç‚¼æ ¸å¿ƒè§‚ç‚¹ã€‚åªè¿”å›çº¯æ–‡æœ¬ã€‚
        
        ç´ æå¦‚ä¸‹ï¼š
        {combined_text}
        """

    # 2. å°è¯•æ–¹æ¡ˆ A: Google Gemini
    try:
        response = gemini_model.generate_content(prompt)
        print("     âœ… Gemini ç”ŸæˆæˆåŠŸ")
        return response.text.strip(), links_data
    
    except Exception as e:
        # 3. å¦‚æœ Gemini å¤±è´¥ (429/500/TimeOut)ï¼Œå¯åŠ¨æ–¹æ¡ˆ B: Alibaba Qwen
        error_msg = str(e)
        if "429" in error_msg or "Quota" in error_msg:
            print(f"     âš ï¸ Gemini é¢åº¦è¶…é™ (429)ã€‚")
        else:
            print(f"     âš ï¸ Gemini å‡ºé”™: {error_msg}")
        
        # è°ƒç”¨é˜¿é‡Œ
        qwen_text = call_qwen_model(prompt)
        if qwen_text:
            return qwen_text.strip(), links_data
        else:
            return None, []

def process_daily_news():
    today_str = datetime.datetime.now().strftime('%Y-%m-%d')
    print(f"ğŸš€ å¼€å§‹ç”Ÿæˆ {today_str} çš„æ—¥æŠ¥ (åŒæ¨¡ç‰ˆ)...")

    for main_menu, sub_menus in MENU_STRUCTURE.items():
        for sub_menu, feeds in sub_menus.items():
            
            print(f"\nğŸ“‚ å¤„ç†æ¿å—: {main_menu} > {sub_menu}")
            
            collected_articles = []
            
            # æŠ“å–ç´ æ
            if sub_menu != "æ¯æ—¥ä¸€ç¬‘":
                for url in feeds:
                    print(f"    æ­£åœ¨è¯»å–: {url} ...")
                    feed = fetch_rss_with_headers(url)
                    if not feed or not feed.entries:
                        print(f"    âš ï¸ æœªæŠ“å–åˆ°å†…å®¹")
                        continue
                    for entry in feed.entries[:3]: 
                        collected_articles.append({
                            "title": entry.title,
                            "link": entry.link,
                            "summary": clean_text(entry.get('summary', '') or entry.get('description', ''))
                        })
                
                if not collected_articles:
                    print("    âš ï¸ æ— ç´ æï¼Œè·³è¿‡")
                    continue
                collected_articles = collected_articles[:6]

            # æ™ºèƒ½æ€»ç»“ (è‡ªåŠ¨åˆ‡æ¢æ¨¡å‹)
            summary_text, links_json = generate_brief_smart(main_menu, sub_menu, collected_articles)
            
            if summary_text:
                data = {
                    "date": today_str,
                    "main_menu": main_menu,
                    "sub_menu": sub_menu,
                    "content": summary_text,
                    "links": links_json
                }
                supabase.table("daily_briefs").insert(data).execute()
                print(f"    âœ… å…¥åº“æˆåŠŸï¼")
            
            # å³ä½¿æœ‰å¤‡èƒï¼Œä¹Ÿç¨å¾®ä¼‘æ¯ä¸€ä¸‹ï¼Œä¿æŒä¼˜é›…
            print("    â˜• ä¼‘æ¯ 5 ç§’...")
            time.sleep(5)

    print("\nğŸ‰ å…¨éƒ¨ä»»åŠ¡å®Œæˆï¼")

if __name__ == "__main__":
    process_daily_news()