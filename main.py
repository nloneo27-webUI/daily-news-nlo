import os
import feedparser
import google.generativeai as genai
from supabase import create_client
from dotenv import load_dotenv
import datetime
import time
import json
import requests
import random
import dashscope
from http import HTTPStatus
from openai import OpenAI
import re

# ================= 1. é…ç½®åŒº =================
load_dotenv()

supabase = create_client(os.environ.get("SUPABASE_URL"), os.environ.get("SUPABASE_KEY"))

# åˆå§‹åŒ– AI
genai.configure(api_key=os.environ.get("GEMINI_API_KEY"))
gemini_model = genai.GenerativeModel('gemini-flash-latest')

deepseek_client = OpenAI(
    api_key=os.environ.get("DEEPSEEK_API_KEY"), 
    base_url="https://api.deepseek.com"
)

dashscope.api_key = os.environ.get("DASHSCOPE_API_KEY")

PROXY_PORT = "7897"

# ================= 2. ç¨³å®šçš„ RSS æº =================
RSS_SOURCES = {
    "æ”¿æ²»": [
        "http://rss.sina.com.cn/news/china/focus15.xml",
        "https://www.zaobao.com.sg/rss/news/china",
        "http://feeds.bbci.co.uk/news/world/rss.xml"
    ],
    "ç»æµ": [
        "http://rss.sina.com.cn/news/finance/chinalist.xml",
        "https://feed.36kr.com/tags/finance",
        "https://www.ftchinese.com/rss/news"
    ],
    "ç§‘æŠ€": [
        "https://www.36kr.com/feed",
        "https://sspai.com/feed",
        "https://www.huxiu.com/rss/0.xml"
    ],
    "AI": [
        "https://www.jiqizhixin.com/rss",
        "https://www.qbitai.com/feed",
        "https://rsshub.app/36kr/search/article/AI"
    ]
}

# ================= 3. ä¿®å¤åçš„å›¾ç‰‡åº“ (çœŸå®é“¾æ¥ï¼Œä¸ä¼š404) =================
FIXED_IMAGES = {
    "æ”¿æ²»": [
        "https://images.unsplash.com/photo-1529101091760-6149d4c46b29?w=800&q=80",
        "https://images.unsplash.com/photo-1575517111839-3a3843ee7f5d?w=800&q=80"
    ],
    "ç»æµ": [
        "https://images.unsplash.com/photo-1611974765270-ca1258634369?w=800&q=80",
        "https://images.unsplash.com/photo-1590283603385-17ffb3a7f29f?w=800&q=80"
    ],
    "ç§‘æŠ€": [
        "https://images.unsplash.com/photo-1518770660439-4636190af475?w=800&q=80",
        "https://images.unsplash.com/photo-1550751827-4bd374c3f58b?w=800&q=80"
    ],
    "AI": [
        "https://images.unsplash.com/photo-1677442136019-21780ecad995?w=800&q=80",
        "https://images.unsplash.com/photo-1620712943543-bcc4688e7485?w=800&q=80"
    ],
    "æ®µå­": [
        "https://images.unsplash.com/photo-1505664194779-8beaceb93744?w=800&q=80"
    ]
}

# ================= 4. å·¥å…·å‡½æ•° =================

def set_proxy(enable=True):
    proxy_url = f"http://127.0.0.1:{PROXY_PORT}"
    if enable:
        os.environ["HTTP_PROXY"] = proxy_url
        os.environ["HTTPS_PROXY"] = proxy_url
    else:
        os.environ.pop("HTTP_PROXY", None)
        os.environ.pop("HTTPS_PROXY", None)

def fetch_rss_with_headers(url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}
    try:
        set_proxy(True)
        resp = requests.get(url, headers=headers, timeout=10)
        resp.encoding = 'utf-8'
        return feedparser.parse(resp.content)
    except:
        try:
            set_proxy(False)
            resp = requests.get(url, headers=headers, timeout=10)
            resp.encoding = 'utf-8'
            return feedparser.parse(resp.content)
        except: return None

def clean_text(text):
    from bs4 import BeautifulSoup
    try: return BeautifulSoup(text, "html.parser").get_text()[:300]
    except: return text[:300]

def extract_image_from_entry(entry):
    if 'media_content' in entry and len(entry.media_content) > 0: return entry.media_content[0]['url']
    if 'links' in entry:
        for link in entry.links:
            if 'image' in link.get('type', ''): return link.href
    content = entry.get('summary', '') + entry.get('content', [{'value': ''}])[0].get('value', '')
    match = re.search(r'<img[^>]+src=["\'](http[^"\']+)["\']', content)
    if match: return match.group(1)
    return None

def get_fallback_image(category):
    # ã€ä¿®å¤ç‚¹ã€‘ä¸å†è¯·æ±‚ source.unsplash.comï¼Œè€Œæ˜¯ä»æœ¬åœ°åˆ—è¡¨éšæœºå–
    images = FIXED_IMAGES.get(category, FIXED_IMAGES["ç§‘æŠ€"])
    return random.choice(images)

def is_url_seen(url):
    try:
        res = supabase.table("news_history").select("id").eq("url", url).execute()
        return len(res.data) > 0
    except: return False

def mark_url_seen(url, title):
    try: supabase.table("news_history").insert({"url": url, "title": title}).execute()
    except: pass

def call_ai_smart(prompt, return_json=False):
    # Gemini
    try:
        set_proxy(True)
        response = gemini_model.generate_content(prompt, request_options={'timeout': 20})
        text = response.text.strip()
        if return_json: return json.loads(text.replace("```json", "").replace("```", "").strip())
        return text
    except:
        # DeepSeek
        try:
            set_proxy(False)
            client = deepseek_client
            resp = client.chat.completions.create(model="deepseek-chat", messages=[{"role": "user", "content": prompt}], stream=False)
            text = resp.choices[0].message.content.strip()
            if return_json: return json.loads(text.replace("```json", "").replace("```", "").strip())
            return text
        except:
            # Qwen
            try:
                set_proxy(False)
                resp = dashscope.Generation.call(model=dashscope.Generation.Models.qwen_turbo, prompt=prompt)
                if resp.status_code == HTTPStatus.OK:
                    text = resp.output.text.strip()
                    if return_json: return json.loads(text.replace("```json", "").replace("```", "").strip())
                    return text
            except: return None

# ================= 5. ä¸šåŠ¡é€»è¾‘ =================

def generate_category_cards(category):
    today_str = datetime.datetime.now().strftime('%Y-%m-%d')
    print(f"\nğŸ“‚ å¤„ç†åˆ†ç±»: {category} ...")
    materials = []
    image_pool = {}

    if category == "æ®µå­":
        prompt_sys = "ä½ æ˜¯ä¸€ä¸ªè„±å£ç§€æ¼”å‘˜ã€‚å†™5ä¸ªå…³äºç§‘æŠ€ã€AIã€èŒåœºã€ç”Ÿæ´»çš„çˆ†ç¬‘æ®µå­ã€‚"
    else:
        feeds = RSS_SOURCES.get(category, [])
        for url in feeds:
            feed = fetch_rss_with_headers(url)
            if not feed or not feed.entries: continue
            print(f"    âœ… æŠ“å–: {url}")
            for entry in feed.entries:
                link = entry.link
                if is_url_seen(link): continue
                title = entry.title
                img = extract_image_from_entry(entry)
                if img: image_pool[title] = img
                materials.append(f"æ ‡é¢˜ï¼š{title}\né“¾æ¥ï¼š{link}\næ‘˜è¦ï¼š{clean_text(entry.get('summary',''))}")
                if len(materials) >= 15: break
            if len(materials) >= 15: break

        if not materials:
            print("    âŒ æ— ç´ æ")
            return
        
        random.shuffle(materials)
        materials = materials[:12]
        prompt_sys = f"ä½ æ˜¯ä¸€ä¸ªæ–°é—»ç¼–è¾‘ã€‚æŒ‘é€‰5æ¡æœ€æœ‰ä»·å€¼çš„ã€{category}ã€‘æ–°é—»ã€‚æ¯æ¡200å­—ï¼Œç®€æ˜æ‰¼è¦ã€‚"

    prompt = f"""{prompt_sys}
    è¯·ä¸¥æ ¼è¿”å› JSON æ•°ç»„æ ¼å¼ï¼š
    [ {{"title": "åŸæ ‡é¢˜", "content": "å†…å®¹...", "url": "é“¾æ¥", "source": "æ¥æºåª’ä½“"}} ]
    ç´ æï¼š{chr(10).join(materials)}"""

    cards_json = call_ai_smart(prompt, return_json=True)
    if cards_json:
        final_cards = []
        for card in cards_json:
            # 1. å…ˆç»™é»˜è®¤å›¾ (è¿™æ˜¯å…³é”®ï¼ç¡®ä¿æ¯æ¡éƒ½æœ‰å›¾)
            card['image'] = get_fallback_image(category)
            
            # 2. å¦‚æœä¹‹å‰æŠ“åˆ°äº†åŸå›¾ï¼Œå°è¯•æ›¿æ¢
            if category != "æ®µå­":
                if card.get('url'): mark_url_seen(card['url'], card['title'])
                for raw_title, raw_img in image_pool.items():
                    if card['title'][:5] in raw_title or raw_title[:5] in card['title']:
                        card['image'] = raw_img
                        break
            final_cards.append(card)

        supabase.table("daily_briefs").insert({"date": today_str, "category": category, "cards": final_cards}).execute()
        print(f"   ğŸ‰ [{category}] å…¥åº“æˆåŠŸ")
        time.sleep(3)

def generate_daily_quote():
    today_str = datetime.datetime.now().strftime('%Y-%m-%d')
    print("âœ¨ ç”Ÿæˆå“²ç†...")
    if supabase.table("daily_quotes").select("id").eq("date", today_str).execute().data: return
    prompt = "éšæœºç”Ÿæˆä¸€å¥å¯Œæœ‰å“²ç†çš„åäººåè¨€æˆ–å†å²ä¸Šçš„ä»Šå¤©ã€‚è¿”å›JSON: {\"content\":..., \"author\":...}"
    data = call_ai_smart(prompt, return_json=True)
    if data: supabase.table("daily_quotes").insert({"date": today_str, "content": data.get("content"), "author": data.get("author")}).execute()

def generate_home_summary():
    today = datetime.datetime.now()
    today_str = today.strftime('%Y-%m-%d')
    print("\nğŸ  ç”Ÿæˆé¦–é¡µæ€»ç»“...")
    # å¹´æœ«é€»è¾‘
    if today.month == 12 and today.day >= 24:
        topics = ["AIé‡å¡‘ä¸–ç•Œ", "å…¨çƒç»æµéœ‡è¡", "å¤ªç©ºæ¢ç´¢", "ç§‘æŠ€ä¼¦ç†"]
        topic = random.choice(topics)
        prompt = f"ä»Šå¤©æ˜¯2025å¹´12æœˆ{today.day}æ—¥ã€‚è¯·ä»¥ã€2025å¹´ç»ˆè¯„è¿°ï¼š{topic}ã€‘ä¸ºé¢˜ï¼Œå†™ä¸€ç¯‡250å­—çš„æ·±åº¦çŸ­è¯„ã€‚åªè¿”å›çº¯æ–‡æœ¬ã€‚"
    else:
        prompt = "å†™ä¸€æ®µ200å­—çš„ä»Šæ—¥å…¨çƒæ–°é—»ç»¼è¿°ã€‚"
    summary = call_ai_smart(prompt)
    if summary:
        supabase.table("daily_briefs").insert({"date": today_str, "category": "é¦–é¡µ", "summary": summary}).execute()

if __name__ == "__main__":
    generate_daily_quote()
    for cat in ["æ”¿æ²»", "ç»æµ", "ç§‘æŠ€", "AI", "æ®µå­"]: generate_category_cards(cat)
    generate_home_summary()
    print("\nğŸš€ å…¨éƒ¨å®Œæˆï¼")