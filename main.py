import os
import feedparser
import google.generativeai as genai
from supabase import create_client
from dotenv import load_dotenv
import datetime
import time
import json
import socket

# ================= 1. åŸºç¡€é…ç½® =================
load_dotenv()
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
GEMINI_KEY = os.environ.get("GEMINI_API_KEY")

# ã€æ ¸å¿ƒé‡ç‚¹ã€‘
# è¿™é‡Œçš„é€»è¾‘æ˜¯ï¼šåªè¦è¿è¡Œè„šæœ¬ï¼Œå°±å…¨ç¨‹æŒ‚ä¸Šä»£ç†ã€‚
# å›½å†…æ–°é—»ä¼šå…ˆç»è¿‡ VPN è½¯ä»¶ï¼ŒVPN è½¯ä»¶ä¼šè‡ªåŠ¨åˆ¤æ–­è®©å®ƒâ€œç›´è¿â€è®¿é—®ï¼Œä¸èµ°æµé‡ã€‚
# å›½å¤–æ–°é—»å’Œ Gemini AI åˆ™ä¼šèµ°ä»£ç†ã€‚
os.environ["HTTP_PROXY"] = "http://127.0.0.1:7897"
os.environ["HTTPS_PROXY"] = "http://127.0.0.1:7897"

# åˆå§‹åŒ–
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
genai.configure(api_key=GEMINI_KEY)
model = genai.GenerativeModel('gemini-flash-latest')

# ================= 2. å®šä¹‰æ–°é—»æº =================
RSS_SOURCES = {
    # â”€â”€â”€ å›½å†…æº (Chinese) â”€â”€â”€
    "Tech_CN": ["https://36kr.com/feed", "https://www.ifanr.com/feed"],
    "Business_CN": ["http://www.ftchinese.com/rss/feed"],
    
    # â”€â”€â”€ å›½å¤–æº (Global) â”€â”€â”€
    "AI_Global": ["https://techcrunch.com/category/artificial-intelligence/feed/"],
    "Tech_Global": ["https://www.theverge.com/rss/index.xml"],
    "Business_Global": ["https://www.cnbc.com/id/10001147/device/rss/rss.html"]
}

# ================= 3. æ ¸å¿ƒé€»è¾‘ =================

def clean_html(text):
    from bs4 import BeautifulSoup
    try:
        return BeautifulSoup(text, "html.parser").get_text()[:3000]
    except:
        return text[:3000]

def process_news():
    print(f"ğŸš€ ä»»åŠ¡å¯åŠ¨ (VPNè§„åˆ™æ¨¡å¼å·²æ¥ç®¡ç½‘ç»œ)")
    print(f"{'='*40}")

    for category, feeds in RSS_SOURCES.items():
        print(f"\nğŸ“‚ åˆ†ç±»: {category}")
        
        # åˆ¤æ–­ä¸€ä¸‹æ˜¯ä¸æ˜¯å›½å†…æºï¼Œä»…ä»…ä¸ºäº†ç»™ AI ä¸‹è¾¾ä¸åŒçš„æŒ‡ä»¤
        # åªè¦åˆ†ç±»åé‡ŒåŒ…å« "CN"ï¼Œæˆ‘ä»¬å°±è®¤ä¸ºæ˜¯å›½å†…æ–°é—»
        is_domestic = "CN" in category 

        for feed_url in feeds:
            print(f"  â””â”€â”€ è¯»å–: {feed_url} ...")
            
            try:
                socket.setdefaulttimeout(15)
                feed = feedparser.parse(feed_url)
                
                if not feed.entries:
                    print("     âš ï¸  ç©ºå†…å®¹æˆ–è¯»å–å¤±è´¥")
                    continue

                for entry in feed.entries[:2]: # æ¯ä¸ªæºå–å‰2æ¡
                    link = entry.link
                    title_raw = entry.title

                    # 1. æ•°æ®åº“æŸ¥é‡
                    try:
                        existing = supabase.table("news").select("id").eq("original_url", link).execute()
                        if existing.data:
                            print(f"     [è·³è¿‡] å·²å­˜åœ¨: {title_raw[:15]}...")
                            continue
                    except Exception as e:
                        print(f"     âš ï¸  æŸ¥åº“å°æ•…éšœ (ä¸å½±å“æµç¨‹): {e}")
                        continue

                    # 2. å‡†å¤‡å‘ç»™ AI çš„å†…å®¹
                    content_raw = entry.get('summary', '') or entry.get('description', '')
                    clean_content = clean_html(title_raw + ". " + content_raw)

                    # 3. åŠ¨æ€æ„å»º AI æŒ‡ä»¤
                    print(f"     âš¡ å‘¼å« Gemini å¤„ç†...")
                    
                    if is_domestic:
                        # å›½å†…æ–°é—»ï¼šåªæ€»ç»“
                        prompt_sys = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šç¼–è¾‘ã€‚å¯¹ä¸­æ–‡æ–°é—»è¿›è¡Œç²¾ç®€æ‘˜è¦ã€‚è¦æ±‚ï¼š1.æ ‡é¢˜ä¿æŒåŸæ„ã€‚2.æ‘˜è¦100å­—ä»¥å†…ï¼Œæå–æ ¸å¿ƒäº‹å®ã€‚3.è¿”å›ä¸¥æ ¼JSONæ ¼å¼ {'title': '...', 'summary': '...'}"
                    else:
                        # å›½å¤–æ–°é—»ï¼šç¿»è¯‘ + æ€»ç»“
                        prompt_sys = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šç¼–è¾‘ã€‚å°†è‹±æ–‡æ–°é—»ç¿»è¯‘å¹¶æ€»ç»“ä¸ºä¸­æ–‡ã€‚è¦æ±‚ï¼š1.æ ‡é¢˜åœ°é“ä¸­æ–‡ã€‚2.æ‘˜è¦100å­—ä»¥å†…ã€‚3.è¿”å›ä¸¥æ ¼JSONæ ¼å¼ {'title': '...', 'summary': '...'}"

                    final_prompt = f"{prompt_sys}\n\næ–°é—»å†…å®¹ï¼š\n{clean_content}"

                    # 4. è°ƒç”¨ AI
                    try:
                        response = model.generate_content(final_prompt)
                        text_resp = response.text.replace("```json", "").replace("```", "").strip()
                        ai_data = json.loads(text_resp)
                        
                        # 5. å…¥åº“
                        new_record = {
                            "title": ai_data.get("title", title_raw),
                            "summary": ai_data.get("summary", "æš‚æ— æ‘˜è¦"),
                            "original_url": link,
                            "source_name": feed.feed.get('title', 'Unknown'),
                            "category": category.split('_')[0], # å»æ‰ _CN æˆ– _Global åç¼€
                            "published_at": datetime.datetime.now().isoformat(),
                            "status": "pending",
                            "image_url": "https://images.unsplash.com/photo-1504711434969-e33886168f5c?auto=format&fit=crop&w=800&q=80"
                        }
                        
                        supabase.table("news").insert(new_record).execute()
                        print(f"     âœ… å…¥åº“æˆåŠŸ: {ai_data.get('title')[:15]}...")
                        
                        time.sleep(10) # ç¨å¾®æ­‡ä¸€ä¼šï¼Œé˜²å°

                    except Exception as e:
                        print(f"     âŒ AIå¤„ç†æˆ–å…¥åº“å¤±è´¥: {e}")
                        continue

            except Exception as e:
                print(f"  âŒ è¯»å–æºå¤±è´¥: {e}")
                continue

    print(f"\nğŸ‰ å…¨éƒ¨å®Œæˆï¼è¯·å» Supabase æŸ¥çœ‹æ•°æ®ã€‚")

if __name__ == "__main__":
    process_news()